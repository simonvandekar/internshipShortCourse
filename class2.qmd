---
title: "Class 2"
author: "Simon Vandekar"
toc: true
format:
  html:
    code-fold: true
    html-math-method: katex
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(GPs=function(before, options, envir){
if (before){
  cex=1.5
par(mgp=c(1.7,.7,0), lwd=1.5, lend=2,
    cex.lab=0.8*cex, cex.axis=0.8*cex, cex.main=0.6*cex,
    mar=c(2.8,2.8,1.8,.2), bty='l', oma=c(0,0,0,0), pch=16)}
})
knitr::opts_chunk$set(echo = TRUE, fig.height = 3, fig.width = 3, cache=FALSE, GPs=TRUE)
set.seed(1333)
cols = c('black', RColorBrewer::brewer.pal(9, 'Set1'))
```

## Objectives:

1. Learn some basic concepts about probabilities
2. Learn to use probability to think about data

## Why probability

* Probability is the language of statistics.
* It is how we can understand data and evaluate the quantitative methods we use.

## Properties of probability
Examples
Coin flip
Dice roll
Dependence/Independence
Continuous example
Expected values/Averages
CDF, PDF


###	Coin flips and diagnoses

* Something that takes only two values can be described with a Bernoulli random variable.
  + Coin flip
  + Diagnosis: schizophrenia, COVID+ or not
  
| Bernoulli values $x$ | probabilities |
| ----------- | ----------- |
| 0 (tails)    |    1-p   |
| 1 (heads)   |   p    |

* What is a sensible value for $p$ for the different Bernoulli variables above?


### Random variables and probability notation
* For $X\sim \text{Be}(\pi)$: X is distributed as ($\sim$ means "is distributed as") [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution), with parameter $p \in (0,1)$, e.g. $p=0.25$.
* Blackboard or capital P is probability, $\mathbb{P}(X=x)$, means the probability the random variable $X$ is equal to the nonrandom value $x$.
* $\mathbb{P}(X = x) = p^x(1-p)^{(1-x)}$. That is the probability mass function (PMF) of a Bernoulli random variable
* Common notation is $f_X(x)$ is the PMF of the random variable $X$. In this case $f_X(x):=p^x(1-p)^{(1-x)}$.
* $:=$ notation means "is defined by."
* A random variable does not have a value in itself... We don't usually talk about $X=0.5$, but $\mathbb{P}(X=0.5)$.

### Probability axioms

We have an intuitive understanding of the basic axioms of probability.

* An event $E$ is something that can occur, e.g. head or tails.
* $F$ is the collection of all events e.g. $F = \{ \{0\}, \{1\} \}$.
* Let $\Omega = {0,1}$ be union of all possible events

The (Kolmogorov) axioms are

1. $\mathbb{P}(X\in E) \ge 0$ -- probability is positive.
2. $\mathbb{P}(X\in \Omega) = 1$ -- probabilities sum to 1 (a person has COVID, or they don't). 
3. For disjoint sets $E_1 \cap E_2 = \varnothing$, $\mathbb{P}(E_1 \cup E_2) = \mathbb{P}(E_1) + \mathbb{P}(E_2)$.

All the other properties of probability can be derived using these three axioms.

### Wordle using the multinomial distribution

* Multinomial can be used for distributions with multiple categories.
  + Die rolls, Wordle number of guesses
  + Likert scales (e.g. [PANSS](https://en.wikipedia.org/wiki/Positive_and_Negative_Syndrome_Scale) in schizophrenia), Tanner staging (puberty; 1-5), Cancer staging (0-4)
* The probability of getting the Wordle in a certain number of guesses.
* Also 6 (or one more) possible values.
* Same family of distribution as the die, except with different *parameters*

$Y=$Number of tries | $\mathbb{P}(Y)$  |
|-------|:----:|
| 1 | 0 |
| 2 | 0.03 |
| 3 | 0.28 |
| 4 | 0.40 |
| 5 | 0.18 |
| 6 | 0.11  |
| 7 | ?? |

<center>
![Wordle results](figures/wordle.jpg){#id .class width=25%}



### Probability density function (PDF)

* The probability density function $f_X(x)$ for a random variable $X$ can be thought of as $\mathbb{P}(X=x)$.
* It is called the probability mass function (PMF) for discrete random variables.

The PDF (probability density function) is the derivative of the CDF and often denoted with a lower case letter $f(x)$.
For discrete random variables the PDF is call the PMF (probability mass function).

```{r, echo=FALSE, fig.cap='Density functions.', fig.width=8, fig.height=2}
layout(matrix(1:4, nrow=1))
x = seq(-3, 3, length.out=1000)
plot(x, dnorm(x), type='l', main='Normal(0,1)', ylab='P(X=x)')
x = seq(0, 10, length.out=1000)
plot(x, dgamma(x, shape=1, rate=1/2), type='l', main='Gamma(1,1/2)', ylab='P(X=x)')
x = seq(0, 15)
plot(x, dpois(x, lambda=5), main='Poisson(5)', type='p', ylab='P(X=x)')
x = seq(0, 1)
plot(x, dbinom(x, size = 1, prob = 1/4), main='Bern(1/4)', type='p', ylab='P(X=x)', ylim=c(0,1))
```

#### Wordle PDF

* Let's draw the PDF for the Wordle distribution


#### Body mass index

From the `insurance` dataset

* BMI is a continuous random variable,
$$
\mathrm{BMI} = \mathrm{weight}/\mathrm{height}^2
$$

* Here's a plot from the insurance dataset.
* It looks approximately normally distributed (not quite).
* I've drawn the normal density on top

```{r}
library(RESI)
lims = hist(insurance$bmi, freq = FALSE, ylim=c(0, .08), main='BMI Histogram', xlab='BMI')
x = seq(min(lims$mids), max(lims$mids), length.out=100)
lines(x, dnorm(x, mean = mean(insurance$bmi),sd=sd(insurance$bmi)))
```

* The normal distribution is 
$$
f_X(x) = \frac{1}{2\pi \sigma^2} e^{-\frac{1}{2\sigma^2} (x - \mu)^2}
$$

* In our dataset the mean is $\hat \mu =$ `r round(mean(insurance$bmi), 2)`, and the standard deviation is $\hat\sigma =$ `r round(sd(insurance$bmi), 2)`.
* We can use $f_X(x)$ to compute probabilities that a person's BMI lies in a particular range.
  + Probability less than or equal to "healthy" -- $\mathbb{P}(X \le 25) = \int^{25}_0 f_X(x)dx$
  + Probability in "healthy range" -- $\mathbb{P}(18 < X \le 25) = \int^{25}_{18} f_X(x)dx$

### Cumulative Distribution Function (CDF)

The cumulative distribution function (CDF), $F_X(x)$, for a random variable $X$ is a function that satisfies
$$
F_X(x) = \mathbb{P}(X \le x).
$$
They are usually functions of parameters. Here are some examples.


```{r, echo=FALSE, eval=TRUE, fig.cap='Some distribution functions.', fig.width=8, fig.height=2}
# Example CDFs (Cumulative distribution functions/Distribution functions)
# F(x) = P(X<=x) \in [0,1]
# properties: nonegative increasing function, F(-\infty) = 0, F(\infty)=1
# Multivariate version "Distribution" function
# Normal
# Gamma
# Poisson

layout(matrix(1:4, nrow=1))
x = seq(-3, 3, length.out=1000)
plot(x, pnorm(x), type='l', main='Normal(0,1)', ylab='P(X<=x)')
x = seq(-1, 10, length.out=1000)
plot(x, pgamma(x, shape=1, rate=1/2), type='l', main='Gamma(1,1/2)', ylab='P(X<=x)')
x = seq(-1, 15, length.out=1000)
plot(x, ppois(x, lambda=5), main='Poisson(5)', type='s', ylab='P(X<=x)')
x = seq(-1, 2, length.out=1000)
plot(x, pbinom(x, size = 1, prob = 1/4), main='Bern(1/4)', type='s', ylab='P(X<=x)')
```

* What are some features that you recognize about these distribution functions?


For discrete random variables the derivative of the CDF does not exist because it is a step function, but the probability mass function is the amount the CDF jumps up at that location, heuristically we can define it as
$$
f(x) = F(x+\Delta x) - F(x),
$$
for an infinitesimal value $\Delta x$.


#### BMI example continued

* CDF can be thought of as a percentile function -- you plug in a BMI value and get out the probability that someone in the population is less than that value.
* Use this [link](https://dqydj.com/bmi-percentile-calculator-united-states/) to compute the probabilities of being
  + Underweight
  + Healthy weight
  + Overweight
  + Obese
* **Hint:** you'll have to use some calculus rules to compute the probabilities.


#### Wordle CDF example

* Let's compare the CDFs of my score and my partner's score.

$y=$Number of tries | $P(Y=y|\text{Simon})$  |  $P(Y=y|\text{Lillie})$ |
|-------|:----:|:----:|
| 1 | 0 | 0 
| 2 | 0.03 | 0.05
| 3 | 0.28 | 0.24
| 4 | 0.40 | 0.44
| 5 | 0.18 | 0.19
| 6 | 0.11  | 0.08



<!-- <center> -->
<!-- ![Wordle results](figures/wordle.jpg){#id .class width=25%} -->

```{r}
wordle1 = c(0, 2, 17, 25, 11, 7)
wordle2 = c(0, 2, 9, 16, 7, 3)
wordle1 = cumsum(wordle1/sum(wordle1))
wordle2 = cumsum(wordle2/sum(wordle2))
x = 0:7
plot(x, c(0,wordle1, 1), main='Wordle CDFs', type='s', ylab='P(X<=x)')
points(x, c(0,wordle2, 1), type='s', col='red')
main = c()
```

* What is the interpretation of this graph?
* Who is doing better (getting the word in fewer guesses)?


### Mean

* The mean might be a nicer way to compare our scores.

The mean is the most common expected value

$$
\mathbb{E} X = \int_{-\inf}^\inf x p(x) dx = \sum_{x}x p(x).
$$

* The integral notation is in the sense of "real analysis" type integrals that can refer to sums or integrals. This is to emphasize that the definition is the same with continuous or discrete random variables.

$y=$Number of tries | $P(Y=y|\text{Simon})$  |  $P(Y=y|\text{Lillie})$ | $y\times P(Y=y|\text{Lillie or Simon})$
|-------|:----:|:----:|:----:|
| 1 | 0 | 0 |
| 2 | 0.03 | 0.05|
| 3 | 0.28 | 0.24|
| 4 | 0.40 | 0.44|
| 5 | 0.18 | 0.19|
| 6 | 0.11  | 0.08|

* $\mathbb{E}X$ is a nonrandom value, called a *parameter*.
* A parameter is something that describes a feature of the distribution of a random variable.
* For the Bernoulli distribution, $p$.
* For multinomial, it's the vector of probabilities.
* For the normal distribution

$$\mathbb{E} X = \int_{-\infty}^\infty x \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{1}{2\sigma^2} (x - \mu)^2} dx = \mu.$$

* Properties include
  + $\mathbb{E}\{ aX + b Y\} = \{ a\mathbb{E}X + b \mathbb{E}Y\}$, for constant values $a$ and $b$.


### Variance

* Another common expectation is the variance

$$
\text{Var}(X) = \mathbb{E} (X - \mathbb{E}X)^2 = \mathbb{E} X^2 - (\mathbb{E} X)^2
$$

* In words, it is the average squared distance of the random variable from its expected value.
* It is a measure of the amount of spread of a random variable

* For the normal distribution

$$
\mathrm{Var}( X ) = \int_{-\infty}^\infty (x-\mu)^2 \frac{1}{2\pi \sigma^2} e^{-\frac{1}{2\sigma^2} (x - \mu)^2} dx = \sigma^2.
$$



* Properties 
  + $\mathrm{Var}(bX-a) = b^2 \mathrm{Var}(X)$.

#### Wordle example  

$y=$Number of tries | $P(Y=y|\text{Simon})$  |  $P(Y=y|\text{Lillie})$ | $y\times P(Y=y|\text{Lillie or Simon})$
|-------|:----:|:----:|:----:|
| 1 | 0 | 0 |
| 2 | 0.03 | 0.05|
| 3 | 0.28 | 0.24|
| 4 | 0.40 | 0.44|
| 5 | 0.18 | 0.19|
| 6 | 0.11  | 0.08|

#### Z-normalization

* Given what we know about properties of mean and variance, we can standardize random variables
* Assume $\mathbb{E} X =\mu$ and $\mathrm{Var}(X) = \sigma^2$. What is the mean and variance of?
$$
(X-\mu)/\sigma
$$
* This is often applied to data using estimates
$$
z_i = (x_i - \bar x)/s
$$
* It makes your data mean zero and variance/SD equal to 1.



### Jointly distributed random variables

*Jointly distributed random variables describe the probability of two things occurring.
* In the insurance dataset, we can consider the probability of having a certain number of children living in a certain area.

```{r}
childByRegion = table(insurance$region, insurance$children)
childByRegion = childByRegion/sum(childByRegion)
knitr::kable(round(childByRegion, 2))
```

* Let $X$ be a random variable denoting number of children
  + Is it categorical/continuous/ordinal?
* Let $Y$ be a random variable denoting region
  + Is it categorical/continuous/ordinal?
* joint distribution is a function of both variables $\mathbb{P}(X=x, Y=y) = f(x,y)$.

* Questions about regional differences and number of children can be determined from the table
* What is the probability of having zero kids and living in the east?
* What is the probability of having zero kids?


### Conditional probabilities

* Conditional probabilities are to express statements dependent on something happening.
* Conditional probabilities are written $\mathbb{P}(X=x \mid Y=y)$
  + What is the probability of having three or more kids given living in the southeast? $\mathbb{P}(X\ge 3 \mid Y=\text{``southeast"} )$
  + What is the probability of living in the Northeast if you have one child?
* Conditional probability can be computed from the joint distribution $\mathbb{P}(X=x\mid Y=y) = \mathbb{P}(X=x, Y=y)/\mathbb{P}(Y=y)$

### Independence

* Two random variables are independent if they factor $\mathbb{P}(X=x, Y=y) = \mathbb{P}(X=x)\mathbb{P}(Y=y)$ for all possible values of $x$ and $y$.
* Intuitively, this means that fixing one variable doesn't affect the distribution of the other.
* Another way to express it is that $\mathbb{P}(X=x\mid Y=y) = \mathbb{P}(X=x)$.
* The table is pretty close to independent.

If the table above were perfectly independent, it might look like this:
```{r}
rs = rowSums(childByRegion)
cs = colSums(childByRegion)
knitr::kable(round(outer(rs, cs), 3))
```


## Connecting data and probability with a random sample



### Statistics is a way to learn about the world from a data set

For now, let's work with the question, "What is the proportion of people in the united states who have used alcohol in the last 30 days?"

1. What is my parameter of interest (target parameter)?
    + Descriptively -- proportion of people who have used alcohol
    + Mathematically -- call this value $p$ unknown value
2. What is my population of interest?
    + Descriptively -- The United States population
    + Quantitatively -- Theoretical distribution defined by the model below in point 4. $\text{Be}(p)$
3. What is a data point?
    + An answer from an individual in the population "yes"/"no" to the survey question. Coded as 1/0.
4. What model can I use to describe how I get a data point?
    + Most obvious one is $X_i \sim \text{Be}(p)$, where $p$ is the parameter in point 1.


### A random sample for the alcohol question

* A random sample is a collection of independent random variables that represent potential data points.
* Let $X_i \sim \text{Be}(p)$ for $i=1,\ldots, n$.
* Our assumption about the population the $X_i$ are drawn from connects the random sample to the parameter of interest.
* Our dataset is assumed to be a single realization of the process we are assuming.

#### Example



 
## Estimates

Note, an **estimate** is a function of the observed sample and it is nonrandom (Why is it non random?). We often use lowercase letters to make that clear
$$
\bar x = n^{-1} \sum_{i=1}^n x_i.
$$

## Estimators

An **estimator** is a function of a random sample.
The goal (usually) is to estimate a parameter of interest (here, $p$).
Let's consider the estimator
$$
\hat p = \bar X = n^{-1} \sum_{i=1}^n X_i,
$$
which we know is pretty reasonable since $\mathbb{E} \bar X = p$.

 * We can study the properties of estimators to learn about how they behave.
 * If we like an estimator we can compute an estimate using our sample.
 * We can use features of our estimator to make probabilistic statements about what might happen if we repeat our study. **Dataset-to-dataset variability**
 

## Parameters, Estimators, Estimates

To reiterate:

* **Parameter** -- Target unknown feature of a population (nonrandom)
* **Estimate** -- Value computed from observed data to approximate the parameter (nonrandom)
* **Estimator** -- A function of a random sample to approximate the parameter

In statistics, probability is used to define and describe the behavior of estimators.



## Practice
